TRAINING/TUNING NOTES:

Legacy:
behaviors:
  Foosbots_MultiRod_5:
    trainer_type: ppo
    hyperparameters:
      batch_size: 2048
      buffer_size: 20480
      learning_rate: 3.0e-4
      beta: 5.0e-3
      epsilon: 0.2
      lambd: 0.95
      num_epoch: 3
      learning_rate_schedule: constant
      beta_schedule: constant
      epsilon_schedule: linear
    network_settings:
      normalize: false
      hidden_units: 256
      num_layers: 2
    reward_signals:
      extrinsic:
        gamma: 0.99
        strength: 1.0
    max_steps: 500000
    time_horizon: 1000
    summary_freq: 10000
    self_play:
        window: 10
        play_against_latest_model_ratio: 0.5
        save_steps: 20000
        swap_steps: 10000
        team_change: 100000
		
		
foosball_test_1:
behaviors:
  Foosbots_MultiRod_5:
    trainer_type: ppo
    hyperparameters:
      batch_size: 5120
      buffer_size: 51200
      learning_rate: 5.0e-5
      beta: 5.0e-3
      epsilon: 0.2
      lambd: 0.95
      num_epoch: 3
      learning_rate_schedule: linear
      beta_schedule: linear
      epsilon_schedule: linear
    network_settings:
      normalize: true
      hidden_units: 512
      num_layers: 2
    reward_signals:
      extrinsic:
        gamma: 0.99
        strength: 1.0
    max_steps: 1000000
    time_horizon: 500
    summary_freq: 10000
    self_play:
        window: 10
        play_against_latest_model_ratio: 0.5
        save_steps: 20000
        swap_steps: 20000
        team_change: 100000
		
		
test 1 showed stronger initial growth but that slowed and it experienced several plateus
hypothesis: increase window parameter to grow towards more "robust" policy
			window 10 ---> 20
			
		
foosball_test_2_1:
behaviors:
  Foosbots_MultiRod_5:
    trainer_type: ppo
    hyperparameters:
      batch_size: 5120
      buffer_size: 51200
      learning_rate: 5.0e-5
      beta: 5.0e-3
      epsilon: 0.2
      lambd: 0.95
      num_epoch: 3
      learning_rate_schedule: linear
      beta_schedule: linear
      epsilon_schedule: linear
    network_settings:
      normalize: true
      hidden_units: 512
      num_layers: 2
    reward_signals:
      extrinsic:
        gamma: 0.99
        strength: 1.0
    max_steps: 1000000
    time_horizon: 500
    summary_freq: 10000
    self_play:
        window: 20
        play_against_latest_model_ratio: 0.5
        save_steps: 20000
        swap_steps: 20000
        team_change: 100000
		

foosball_test_3_2:
behaviors:
  Foosbots_MultiRod_5:
    trainer_type: ppo
    hyperparameters:
      batch_size: 5120
      buffer_size: 51200
      learning_rate: 5.0e-5
      beta: 5.0e-3
      epsilon: 0.2
      lambd: 0.9
      num_epoch: 3
      learning_rate_schedule: linear
      beta_schedule: linear
      epsilon_schedule: linear
    network_settings:
      normalize: true
      hidden_units: 512
      num_layers: 2
    reward_signals:
      extrinsic:
        gamma: 0.99
        strength: 1.0
    max_steps: 1000000
    time_horizon: 500
    summary_freq: 10000
    self_play:
        window: 10
        play_against_latest_model_ratio: 0.5
        save_steps: 20000
        swap_steps: 20000
        team_change: 100000
		
foosball_test_4_1
behaviors:
  Foosbots_MultiRod_5:
    trainer_type: ppo
    hyperparameters:
      batch_size: 5120
      buffer_size: 51200
      learning_rate: 5.0e-4
      beta: 5.0e-3
      epsilon: 0.2
      lambd: 0.9
      num_epoch: 3
      learning_rate_schedule: linear
      beta_schedule: linear
      epsilon_schedule: linear
    network_settings:
      normalize: true
      hidden_units: 512
      num_layers: 2
    reward_signals:
      extrinsic:
        gamma: 0.99
        strength: 1.0
    max_steps: 1000000
    time_horizon: 500
    summary_freq: 10000
    self_play:
        window: 10
        play_against_latest_model_ratio: 0.5
        save_steps: 20000
        swap_steps: 20000
        team_change: 100000
		

foosball_test_5
behaviors:
  Foosbots_MultiRod_5:
    trainer_type: ppo
    hyperparameters:
      batch_size: 5120
      buffer_size: 51200
      learning_rate: 5.0e-3
      beta: 5.0e-3
      epsilon: 0.2
      lambd: 0.9
      num_epoch: 3
      learning_rate_schedule: linear
      beta_schedule: linear
      epsilon_schedule: linear
    network_settings:
      normalize: true
      hidden_units: 512
      num_layers: 2
    reward_signals:
      extrinsic:
        gamma: 0.99
        strength: 1.0
    max_steps: 1000000
    time_horizon: 500
    summary_freq: 10000
    self_play:
        window: 10
        play_against_latest_model_ratio: 0.5
        save_steps: 20000
        swap_steps: 20000
        team_change: 100000
		
foosball_test_6_1
behaviors:
  Foosbots_MultiRod_5:
    trainer_type: ppo
    hyperparameters:
      batch_size: 5120
      buffer_size: 51200
      learning_rate: 5.0e-3
      beta: 5.0e-3
      epsilon: 0.2
      lambd: 0.95
      num_epoch: 3
      learning_rate_schedule: linear
      beta_schedule: linear
      epsilon_schedule: linear
    network_settings:
      normalize: true
      hidden_units: 512
      num_layers: 2
    reward_signals:
      extrinsic:
        gamma: 0.99
        strength: 1.0
    max_steps: 1000000
    time_horizon: 500
    summary_freq: 10000
    self_play:
        window: 10
        play_against_latest_model_ratio: 0.5
        save_steps: 20000
        swap_steps: 20000
        team_change: 100000